---
title: "「ゼロから作るDeep Learning」を読んだ"
excerpt: "🐟 深層学習に入門するときの必携書と言っていいと思う"
date: "2022.05.19"
---

# はじめに

先日、卒業研究の練習として深層学習を扱うことになりました。そこで深層学習について調べていると、こんな内容を見つけました。

> ゼロから作る Deep Learning ―Python で学ぶディープラーニングの理論と実装 (斎藤 康毅著) は必携です。この本を読まずして、参照せずして人に質問してはいけません。
>
> ([初めての深層学習ロードマップ [随時更新&文献追加 予定] - Qiita](https://qiita.com/hiroyuki827/items/401517c22c1138492031)より)

マジか。

ということで、深層学習の基本的な知識を身に着けるために、[ゼロから作る Deep Learning――Python で学ぶディープラーニングの理論と実装](https://www.oreilly.co.jp/books/9784873117584/)を買って読みました。

https://www.oreilly.co.jp/books/9784873117584/

今回は、各章のおおまかな内容と感想をメインに書いていこうと思います。

# 言いたいこと

**深層学習に入門するならやっとけ**

# 1 章 Python 入門

ゼロから「作る」というタイトルですが、今回は実装などは行わずに知識や用語をメインで学習しました。Python の基本的な文法は知っていましたし、他の書籍で実装の部分を学習するつもりだったので、第 1 章はほぼ流し読みになりました。

個人的には、NumPy におけるブロードキャストは初めて知りました。NumPy を扱う上でブロードキャストがどういう処理になるのかを、図を用いて説明してありました。そのため、初見の流し読みでも頭に入ってきやすかったです。

# 2 章 パーセプトロン

この章では、パーセプトロンアルゴリズムについて説明されていました。入力に応じて決まった値を出力するパーセプトロンにおいて、「重み」や「バイアス」の意味・閾値の理解・層を増やすことによる表現力の増強などが主な内容でした。

この章で私が特に参考になったのは、次の 2 点です。

- 「重み」とは、 **入力信号への重要度** をコントロールするパラメータである
- 「バイアス」とは、 **発火のしやすさ** をコントロールするパラメータである

機械学習の文脈でも、この「重み」「バイアス」という単語は聞いたことがあったので、概念的な意味をしっかり捉えられたのは良かったです。

# 3 章 ニューラルネットワーク

この章では、ニューラルネットワークについて説明されていました。「パーセプトロンでは人間がやっていた重みパラメータの調整を自動でやってくれる」ということで、ニューラルネットワークがどういう点で優れているのかを具体的に理解できたのが良かったです。

この辺りから初見の知識がどんどん出てきたので、内容を理解するのに時間がかかりました。この章で特に参考になったのは

- 「活性化関数」とは、 **入力信号の総和を出力信号に変換する関数** である
- パーセプトロンではステップ関数を使う一方、ニューラルネットワークではソフトマックス関数・ReLU 関数を使う
- 行列計算に落とし込むと、各層における複数の計算が一気にできる
- ソフトマックス関数は、出力を「確率」と解釈することができる
- 「正規化」とは、 **データを決まった値の範囲に収まるように変換する** 処理のことである

辺りです。

前章を踏まえて、ニューラルネットワークが具体的にパーセプトロンとどう違うのか理解できました。機械学習で一般的に知られている（と思う）概念も把握できたので、とても良かったです。

# 4 章 ニューラルネットワークの学習

この章では、ニューラルネットワークの学習が具体的にどういう処理を行っているのか説明されていました。

特に参考になったのは次の通りです。

- 「損失関数」とは、 **学習用データと比べてどれくらい差があるのか(性能の悪さ)** を示す指標である
- ニューラルネットワークの学習においては、損失関数の値をできるだけ小さくする方向に重みパラメータを調整する
- 損失関数の値を小さくする際、 **損失関数の微分(勾配)** を手掛かりにする
- 学習率とは、 **どれだけ学習する（重みパラメータを更新する）べきか** を決めるパラメータである

「TensorFlow などの深層学習フレームワークを使うときに、ここら辺の知識ないとしんどいだろうなぁ」[^1]と思いつつ、読み進めていきました。

# 5 章 誤差逆伝播法

数値微分による勾配の計算は、一般的にとても時間がかかります。これを解決する方法として、重みパラメータの勾配を効率よく計算できる誤差逆伝播法[^2]が説明されていました。

初見では、誤差逆伝播法はあまり理解できませんでした。順伝播と逆の方向に微分を繰り返すことで、どういう値を求められるのかがイメージできませんでした。

そこで誤差逆伝搬法について調べてみたところ、次のような内容を見つけました。参考程度に紹介しておきます。

:::details 誤差逆伝搬法の解釈

渓流がたくさんあります。

流れの強さは、緩やかなものもあれば急なものもあり、流れの向きは順速(加速する)のものもあれば逆速(減速する)のものもあります。

上流から岩石を下流に向けて流すと、全ての滝を超えて下流についた頃には隕石のスピードになったとします。

次に、全ての滝に関して、流れの勢いは変えないまま、流れの向きだけを反対にします。そして今度は下流から上流に岩石を流したとします。

全ての滝を超えて上流についた頃には同じく隕石のスピードになっています。

**これが逆伝播の理屈です。**

順伝播と逆伝播の違いは、渓流を上流から下流に向かうか、下流から上流に向かうかのみの違いで、準速の渓流は準速のまま、逆速の渓流は逆速のまま計算しているため、結局、岩石が渓流を渡る順番が違うだけなのです。

したがって、順伝播も逆伝播も計算結果は同じで、どちらの方法でも岩石は同じスピードで帰ってくる。

要は、逆方向に進んではいるが、やってることは順伝播と一緒なのです。

([ゼロから作る Deep Learning 第 5 章](https://www.dragonarrow.work/articles/176)より)

:::

**順方向に計算しても逆方向に計算しても結果は同じになる**ということです。事実、上流ノードと下流ノードに同じ値を渡すと、順伝播でも逆伝播でも計算結果が一緒になります。

上流ノードに渡された値は、足し算ノードや乗算ノードをたどって、下流に向かって次々と計算することができました。この要領で、下流ノードには微分値を渡して、各ノードをたどって、上流に向かって次々と微分することができます。

上流に値を渡せるという性質が、ニューラルネットワークの学習において必要になるというわけですね。

他にも、以下のような内容が参考になりました。

- ニューラルネットワークの構成要素を「レイヤ」としてモジュール化することで、ニューラルネットワークをレゴブロックのように簡単に実装できる
- 各レイヤの内部で順伝播や逆伝播が行われることで、認識処理や学習に必要な勾配が正しく求められる

# 第 6 章 学習に関するテクニック

この章では、ニューラルネットワークの学習におけるさまざまなテーマについて説明されていました。この各テーマにおいて、参考になった内容をいくつか載せておきます

## 重みの初期値

- 重みの初期値によって、ニューラルネットワークの学習結果が大きく変わる
- 現在の深層学習フレームワークでは、Xavier の初期値という初期値が標準的に用いられている
  - 前層のノードの個数を $n$ としたとき、 $\frac{1}{\sqrt{n}}$ の標準偏差を持つ分布を使う
- ReLU を使うときは、He の初期値という初期値を使う
  - 前層のノードの個数を $n$ としたとき、 $\sqrt{\frac{2}{n}}$ の標準偏差を持つ分布を使う

深層学習に深くかかわってくる部分なので、頭の片隅に入れておいて、必要な時に選択できるようになりたいですね。

## Match Normalization

- Batch Normalization を使うと、学習が早くなる
- Batch Normalization を使うと、初期値にそれほど依存しなくなる

基本的にいい事しかないので、積極的に使っていけますね。

## 正則化

- 過学習を押さえる方法として、Weight decay(荷重減衰)という手法がある
  - Weight decay では、大きすぎる重みを持つことに対してペナルティを与える
- 他にも、Dropout という手法もある
  - Dropout では、ニューロンをランダムに消去しながら学習する

過学習は学習結果の信頼性に大きく影響するので、しっかり対策する必要がありますね。

# 第 7 章 畳み込みニューラルネットワーク

ここでは前章のニューラルネットワークを発展させて、畳み込みニューラルネットワーク(CNN)を扱っていきます。

今回は参考になった内容はこんな感じです。

- CNN は、ニューラルネットワークに「Convolution レイヤ(畳み込み層)」と「Pooling レイヤ(プーリング層)」が加わったもの
- **形状のあるデータ（3 次元のデータなど）** を扱うことができる
- CNN のフィルターパラメータは、ニューラルネットワークでは重みと一緒

畳み込みニューラルネットワークという単語自体は聞いたことがありました。しかし、ニューラルネットワークとの違いとして、画像など形状のあるデータを扱えるということは知りませんでした。画像を扱えるようにしたニューラルネットワークという感じですね。

# 第 8 章ディープラーニング

本書の最後の章として、深層学習の性質や課題に触れていきます。

深層学習は、ニューラルネットワークの層をより深くしたものです。具体的には、以下のような内容を学んでいきました。

- 層をより深くすることで、ニューラルネットワークの制度を向上させることができる
- 層をより深くすることで、学習すべき問題を階層的に分解することもできる
- 深層学習が現在発展中の分野であり、さまざまな研究がなされている

個人的には、画像をゼロから生成する DCGAN などが面白そうと思いました。

# おわりに

全体を通して、読書感想文みたいな内容になってしまいました。しかし、「**深層学習に取り組む上で常識として知っているであろう知識**」については、一通り学ぶことができたかなと感じます。これから卒業研究が始まりますが、本書の内容を活かすも殺すも自分次第なので、まあ頑張ろうかなと思います。

# 参考文献

- [初めての深層学習ロードマップ [随時更新&文献追加 予定] - Qiita](https://qiita.com/hiroyuki827/items/401517c22c1138492031)
- [ゼロから作る Deep Learning 第 5 章](https://www.dragonarrow.work/articles/176)

[^1]: 色々な記事に「フレームワークの使い方だけ勉強しても、中身を理解しないとしんどくなる」と書いてあったので...
[^2]: 伝搬(でんぱん)ではなく伝播(でんぱ)です
